# IDS Classifier Model

This directory contains the machine learning model training and inference code for the Intrusion Detection System (IDS) classifier.

## Overview

The IDS classifier is a **Decision Tree** model trained on network traffic data to distinguish between normal and malicious (attack) traffic. The model uses a comprehensive feature engineering pipeline and achieves high accuracy on the test set.

## Architecture

```
┌─────────────────┐       ┌──────────────────┐       ┌─────────────────┐
│  Raw Network    │ ───▶  │  Preprocessor    │ ───▶  │ Decision Tree   │
│  Traffic Data   │       │  Pipeline        │       │  Classifier     │
└─────────────────┘       └──────────────────┘       └─────────────────┘
                                  │                           │
                                  ▼                           ▼
                          ┌──────────────────┐       ┌─────────────────┐
                          │ Feature          │       │  Prediction:    │
                          │ Transformation:  │       │  Normal/Attack  │
                          │ - OneHot Encode  │       │  + Confidence   │
                          │ - MinMax Scale   │       └─────────────────┘
                          │ - Log2 Transform │
                          └──────────────────┘
```

## Files

### Core Modules

- **`train.py`**: Complete training pipeline
  - Downloads IDS dataset from Kaggle
  - Performs feature engineering and preprocessing
  - Trains Decision Tree classifier
  - Evaluates with cross-validation
  - Saves all model artifacts

- **`inference.py`**: Production inference module
  - Loads trained model and preprocessor
  - Provides single-sample and batch prediction
  - Used by the classifier service

### Model Artifacts (Generated by train.py)

```
models/
├── model.joblib              # Trained Decision Tree classifier
├── preprocessor.joblib       # ColumnTransformer for feature preprocessing
├── drop_columns.joblib       # List of low-frequency columns to drop
├── model_metadata.joblib     # Model metadata (metrics, hyperparameters, timestamp)
└── feature_importance.csv    # Feature importance scores
```

## Features

### Input Features (41 total)

The model uses 41 features from network connection records:

**Categorical Features (4):**
- `protocol_type`: tcp, udp, icmp
- `service`: http, smtp, ftp, etc.
- `flag`: connection status (SF, REJ, etc.)
- `logged_in`: whether user logged in (0/1)

**Numerical Features (37):**
- **Basic:** duration, src_bytes, dst_bytes
- **Content:** hot, num_failed_logins, num_compromised, etc.
- **Time-based:** count, srv_count, serror_rate, srv_serror_rate, etc.
- **Host-based:** dst_host_count, dst_host_srv_count, etc.

### Feature Engineering

1. **One-Hot Encoding**: Categorical features → binary columns
2. **MinMax Scaling**: Count features normalized to [0, 1]
3. **Log2 Transformation**: Byte features (src_bytes, dst_bytes) with log2 scaling
4. **Low-Frequency Filtering**: Drop columns with <50 (service) or <10 (flag) occurrences

## Model Details

### Hyperparameters

```python
{
    "model": "DecisionTreeClassifier",
    "criterion": "entropy",
    "max_depth": 6,
    "random_state": 635458
}
```

### Performance Metrics

Typical performance on the test set:

| Metric    | Score  |
|-----------|--------|
| Accuracy  | ~0.99  |
| Precision | ~0.99  |
| Recall    | ~0.99  |
| F1 Score  | ~0.99  |

**Note:** Actual metrics may vary slightly. Run `train.py` to see exact results.

### Data Split

- **Training**: 49% (for model training)
- **Validation**: 30% (for evaluation during training)
- **Test**: 21% (for final evaluation)

All splits are stratified by the target variable (`is_attack`) to maintain class balance.

## Usage

### Training a New Model

```bash
# From project root
python src/model/train.py
```

**Requirements:**
- Kaggle API credentials configured (for dataset download)
- Python 3.11+
- Required packages: scikit-learn, pandas, numpy, joblib, kagglehub

**Output:**
- Prints training progress and metrics
- Saves all artifacts to `models/` directory
- Displays cross-validation results, validation metrics, test metrics
- Shows top 10 most important features

### Using the Model for Inference

```python
from src.model.inference import IDSClassifier

# Initialize classifier (loads artifacts from models/)
classifier = IDSClassifier()

# Predict on a single sample
sample = {
    "duration": 0,
    "protocol_type": "tcp",
    "service": "http",
    "flag": "SF",
    "src_bytes": 181,
    "dst_bytes": 5450,
    # ... other features
}

result = classifier.predict_one(sample)
print(result)
# {
#   "prediction": "normal",
#   "prediction_label": 0,
#   "confidence": 0.92,
#   "probabilities": {"normal": 0.92, "attack": 0.08}
# }
```

### Inference Demo

```bash
# Run the built-in demo
python src/model/inference.py
```

## Integration with Classifier Service

The classifier service (`src/services/classifier/`) uses this model for real-time inference:

1. Service loads artifacts at startup using `IDSClassifier`
2. Consumes traffic messages from Redis queue
3. Extracts features and calls `classifier.predict_one(features)`
4. Publishes results to Redis pub/sub channel

**Volume Mounting:**
```yaml
# docker-compose.yml
services:
  classifier:
    volumes:
      - ./models:/app/models  # Mount trained model artifacts
```

## Development Workflow

### 1. Train Initial Model

```bash
python src/model/train.py
```

### 2. Verify Artifacts

Check that all required files exist:
```bash
ls -lh models/
# Should show: model.joblib, preprocessor.joblib, drop_columns.joblib, model_metadata.joblib
```

### 3. Test Inference

```bash
python src/model/inference.py
```

### 4. Integrate with Service

Start the classifier service with model artifacts:
```bash
docker-compose up classifier
```

## Customization

### Changing Hyperparameters

Edit `train.py` configuration:

```python
# Configuration
MODEL_CRITERION = "gini"      # or "entropy"
MODEL_MAX_DEPTH = 8           # tree depth
RANDOM_SEED = 42              # for reproducibility
```

### Using a Different Algorithm

Replace `DecisionTreeClassifier` with another sklearn classifier:

```python
from sklearn.ensemble import RandomForestClassifier

def train_model(x_train, y_train):
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=RANDOM_SEED
    )
    model.fit(x_train, y_train)
    return model
```

**Important:** Update `model_metadata` to reflect the new model type.

### Feature Engineering

Modify the preprocessing pipeline in `create_preprocessor()`:

```python
preprocessor = ColumnTransformer(
    transformers=[
        ("onehot", OneHotEncoder(...), categorical_cols),
        ("standard", StandardScaler(), numerical_cols),  # Changed from MinMax
        # Add more transformers...
    ]
)
```

## Troubleshooting

### Kaggle Dataset Download Fails

**Error:** `OSError: Could not find kaggle.json`

**Solution:**
1. Create Kaggle API credentials at https://www.kaggle.com/account
2. Download `kaggle.json`
3. Place in `~/.kaggle/kaggle.json` (Linux/Mac) or `%USERPROFILE%\.kaggle\kaggle.json` (Windows)
4. Set permissions: `chmod 600 ~/.kaggle/kaggle.json`

### Model File Not Found

**Error:** `FileNotFoundError: Model file not found at models/model.joblib`

**Solution:**
Run training script first:
```bash
python src/model/train.py
```

### Low Model Performance

**Symptoms:** Accuracy < 0.90

**Possible Causes:**
- Insufficient tree depth (try increasing `MODEL_MAX_DEPTH`)
- Wrong criterion (try switching between 'gini' and 'entropy')
- Data leakage (verify train/test split is correct)

**Debug:**
1. Check feature importance: `cat models/feature_importance.csv`
2. Review validation metrics in training output
3. Verify class balance in splits

## Testing

Unit tests for the model are not yet implemented. Future work could include:
- Test preprocessing pipeline
- Test model serialization/deserialization
- Test inference with known samples
- Integration tests with classifier service

## Performance Considerations

### Training Time

- **Dataset size**: ~125,000 records
- **Training time**: ~1-2 minutes on standard hardware
- **Memory**: ~500MB during training

### Inference Time

- **Single prediction**: <1ms
- **Batch (1000 samples)**: ~50ms
- **Model size on disk**: ~500KB

### Scaling

- Decision Trees are fast for inference (O(log n) depth)
- Model can handle ~10,000 predictions/second
- No GPU required

## References

- **Dataset**: [Kaggle - Intrusion Detection Classifier](https://www.kaggle.com/datasets/ayushparwal2026/intrusion-detection-classifier)
- **Algorithm**: Scikit-learn DecisionTreeClassifier
- **Metrics**: Accuracy, Precision, Recall, F1-Score

## Future Improvements

1. **Hyperparameter Tuning**: Use GridSearchCV or RandomizedSearchCV
2. **Ensemble Methods**: Try Random Forest or Gradient Boosting
3. **Feature Selection**: Use RFECV to reduce feature count
4. **Model Versioning**: Add MLflow for experiment tracking
5. **Online Learning**: Implement incremental training for model updates
6. **Explainability**: Add SHAP values for prediction explanations
